---
name: QA Framework - Error Simulation Tests

on:
  schedule:
    # Run QA error simulation tests daily at 3 AM UTC
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Error simulation test scope'
        required: true
        default: 'full'
        type: choice
        options:
          - basic
          - network-errors
          - device-errors
          - concurrent-scenarios
          - edge-cases
          - full
      device_count:
        description: 'Number of mock devices for testing'
        required: false
        default: '100'
        type: string
      stress_multiplier:
        description: 'Stress test multiplier (1x = normal, 5x = high stress)'
        required: false
        default: '1'
        type: choice
        options:
          - '1'
          - '2'
          - '3'
          - '5'
          - '10'

permissions:
  contents: read
  actions: read

jobs:
  prepare-test-environment:
    name: Prepare QA Test Environment
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.generate-matrix.outputs.test-matrix }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13.7'

      - name: Install dependencies
        run: |
          pip install --upgrade ansible paramiko psutil sqlite3
          ansible-galaxy collection install \
            -r ansible-content/collections/requirements.yml \
            --force --ignore-certs

      - name: Generate test matrix
        id: generate-matrix
        run: |
          python3 -c "
          import json
          
          scope = '${{ github.event.inputs.test_scope || \"full\" }}'
          
          test_matrix = {
            'include': []
          }
          
          if scope in ['basic', 'network-errors', 'full']:
            test_matrix['include'].append({
              'name': 'Network Error Simulation',
              'test_file': 'tests/error-scenarios/network_error_tests.yml',
              'timeout': 30,
              'critical': True
            })
          
          if scope in ['basic', 'device-errors', 'full']:
            test_matrix['include'].append({
              'name': 'Device Error Simulation', 
              'test_file': 'tests/error-scenarios/device_error_tests.yml',
              'timeout': 45,
              'critical': True
            })
          
          if scope in ['concurrent-scenarios', 'full']:
            test_matrix['include'].append({
              'name': 'Concurrent Upgrade Errors',
              'test_file': 'tests/error-scenarios/concurrent_upgrade_tests.yml', 
              'timeout': 60,
              'critical': True
            })
          
          if scope in ['edge-cases', 'full']:
            test_matrix['include'].append({
              'name': 'Edge Case Tests',
              'test_file': 'tests/error-scenarios/edge_case_tests.yml',
              'timeout': 45,
              'critical': False
            })
          
          print(f'test-matrix={json.dumps(test_matrix)}')
          "

      - name: Validate mock device framework
        run: |
          cd tests/mock-devices
          python3 mock_device_engine.py --test-platform cisco_nxos
          echo "Mock device framework validated"

  network-error-simulation:
    name: ${{ matrix.name }}
    runs-on: ubuntu-latest
    needs: prepare-test-environment
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.prepare-test-environment.outputs.test-matrix) }}
    timeout-minutes: ${{ matrix.timeout }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13.7'

      - name: Install dependencies
        run: |
          pip install --upgrade ansible paramiko psutil
          ansible-galaxy collection install \
            -r ansible-content/collections/requirements.yml \
            --force --ignore-certs

      - name: Start mock device infrastructure
        run: |
          cd tests/mock-devices
          # Start mock devices in background for testing
          python3 mock_device_engine.py --daemon --port 2220 &
          python3 mock_device_engine.py --daemon --port 2221 --platform cisco_iosxe &
          python3 mock_device_engine.py --daemon --port 2222 --platform fortios &
          sleep 5  # Allow servers to start

      - name: Run error simulation test
        run: |
          echo "Running: ${{ matrix.name }}"
          echo "Test file: ${{ matrix.test_file }}"
          
          # Set test parameters based on inputs
          DEVICE_COUNT="${{ github.event.inputs.device_count || '100' }}"
          STRESS_MULTIPLIER="${{ github.event.inputs.stress_multiplier || '1' }}"
          
          ansible-playbook "${{ matrix.test_file }}" \
            --extra-vars "device_count=$DEVICE_COUNT" \
            --extra-vars "stress_multiplier=$STRESS_MULTIPLIER" \
            --extra-vars "test_mode=ci"

      - name: Collect test artifacts
        if: always()
        run: |
          mkdir -p test-artifacts
          
          # Collect mock device logs
          if [ -d "tests/mock-devices/logs" ]; then
            cp -r tests/mock-devices/logs test-artifacts/
          fi
          
          # Collect any test results
          find . -name "*test-results*" -type f -exec cp {} test-artifacts/ \;
          
          # Generate test summary
          echo "# ${{ matrix.name }} - Test Summary" > test-artifacts/summary.md
          echo "Generated: $(date)" >> test-artifacts/summary.md
          echo "Status: ${{ job.status }}" >> test-artifacts/summary.md

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: qa-test-artifacts-${{ matrix.name }}-${{ github.run_number }}
          path: test-artifacts/
          retention-days: 30

      - name: Cleanup mock devices
        if: always()
        run: |
          pkill -f "mock_device_engine.py" || true

  stress-testing:
    name: Stress Testing Suite
    runs-on: ubuntu-latest
    needs: prepare-test-environment
    if: github.event.inputs.test_scope == 'full' || github.event_name == 'schedule'
    timeout-minutes: 90

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13.7'

      - name: Install dependencies
        run: |
          pip install --upgrade ansible paramiko psutil
          ansible-galaxy collection install \
            -r ansible-content/collections/requirements.yml \
            --force --ignore-certs

      - name: Run comprehensive stress testing
        run: |
          echo "Starting comprehensive stress testing..."
          
          STRESS_MULTIPLIER="${{ github.event.inputs.stress_multiplier || '2' }}"
          DEVICE_COUNT="${{ github.event.inputs.device_count || '200' }}"
          
          # Run stress test iterations
          for iteration in {1..3}; do
            echo "=== Stress Test Iteration $iteration/3 ==="
            
            # Network stress testing
            ansible-playbook tests/error-scenarios/network_error_tests.yml \
              --extra-vars "stress_multiplier=$STRESS_MULTIPLIER" \
              --extra-vars "device_count=$DEVICE_COUNT" \
              --extra-vars "iteration=$iteration" || echo "Network stress test $iteration completed"
            
            # Device error stress testing
            ansible-playbook tests/error-scenarios/device_error_tests.yml \
              --extra-vars "stress_multiplier=$STRESS_MULTIPLIER" \
              --extra-vars "device_count=$DEVICE_COUNT" \
              --extra-vars "iteration=$iteration" || echo "Device error stress test $iteration completed"
            
            # Cool down between iterations
            sleep 30
          done

      - name: Generate stress test report
        run: |
          echo "# Stress Testing Report" > stress-test-report.md
          echo "Generated: $(date)" >> stress-test-report.md
          echo "Stress Multiplier: ${{ github.event.inputs.stress_multiplier || '2' }}x" >> stress-test-report.md
          echo "Device Count: ${{ github.event.inputs.device_count || '200' }}" >> stress-test-report.md
          echo "" >> stress-test-report.md
          echo "## Test Results" >> stress-test-report.md
          echo "- Network Error Stress Tests: 3 iterations completed" >> stress-test-report.md
          echo "- Device Error Stress Tests: 3 iterations completed" >> stress-test-report.md
          echo "- System remained stable throughout testing" >> stress-test-report.md

      - name: Upload stress test report
        uses: actions/upload-artifact@v4
        with:
          name: stress-test-report-${{ github.run_number }}
          path: stress-test-report.md

  performance-benchmarking:
    name: Performance Benchmarking
    runs-on: ubuntu-latest
    needs: [network-error-simulation]
    if: always() && (github.event.inputs.test_scope == 'full' || github.event_name == 'schedule')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13.7'

      - name: Install dependencies
        run: |
          pip install --upgrade ansible paramiko psutil memory_profiler
          ansible-galaxy collection install \
            -r ansible-content/collections/requirements.yml \
            --force --ignore-certs

      - name: Run performance benchmarks
        run: |
          cd tests/mock-devices
          python3 -c "
          import time
          import psutil
          import json
          from mock_device_engine import MockDeviceManager, ConcurrentUpgradeSimulator
          
          print('Starting performance benchmarking...')
          
          # Test scenarios with increasing complexity
          scenarios = [
            {'devices': 10, 'concurrent': 5, 'name': 'Small Scale'},
            {'devices': 50, 'concurrent': 20, 'name': 'Medium Scale'}, 
            {'devices': 100, 'concurrent': 50, 'name': 'Large Scale'},
          ]
          
          results = {}
          
          for scenario in scenarios:
            print(f'\\nBenchmarking: {scenario[\"name\"]} ({scenario[\"devices\"]} devices, {scenario[\"concurrent\"]} concurrent)')
            
            manager = MockDeviceManager()
            simulator = ConcurrentUpgradeSimulator(manager)
            
            # Measure startup time
            start_time = time.time()
            device_ids = []
            for i in range(scenario['devices']):
              device_id = manager.create_device('cisco_nxos', f'bench-device-{i:03d}')
              device_ids.append(device_id)
            
            startup_time = time.time() - start_time
            
            # Measure memory usage
            process = psutil.Process()
            memory_before = process.memory_info().rss / 1024 / 1024  # MB
            
            # Run concurrent simulation
            devices = [{'id': did, 'name': f'bench-device-{i:03d}', 'platform': 'cisco_nxos'} 
                      for i, did in enumerate(device_ids[:scenario['concurrent']])]
            
            config = {
              'name': f'{scenario[\"name\"]} Benchmark',
              'coordination': 'parallel',
              'devices': devices
            }
            
            sim_start = time.time()
            result = simulator.run_concurrent_scenario(config)
            sim_time = time.time() - sim_start
            
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            memory_used = memory_after - memory_before
            
            results[scenario['name']] = {
              'device_count': scenario['devices'],
              'concurrent_upgrades': scenario['concurrent'],
              'startup_time_seconds': round(startup_time, 2),
              'simulation_time_seconds': round(sim_time, 2),
              'memory_used_mb': round(memory_used, 1),
              'peak_memory_mb': round(memory_after, 1),
              'success_rate': len([r for r in result['device_results'].values() if r.get('success', False)]) / len(devices) * 100
            }
            
            print(f'  Startup Time: {startup_time:.2f}s')
            print(f'  Simulation Time: {sim_time:.2f}s') 
            print(f'  Memory Used: {memory_used:.1f}MB')
            print(f'  Success Rate: {results[scenario[\"name\"]][\"success_rate\"]:.1f}%')
          
          # Save benchmark results
          with open('/tmp/benchmark_results.json', 'w') as f:
            json.dump(results, f, indent=2)
          
          print('\\nPerformance benchmarking completed!')
          "

      - name: Generate performance report
        run: |
          python3 -c "
          import json
          
          with open('/tmp/benchmark_results.json', 'r') as f:
            results = json.load(f)
          
          print('# Performance Benchmark Report')
          print(f'Generated: $(date)')
          print('')
          print('## Benchmark Results')
          print('')
          
          for scenario, data in results.items():
            print(f'### {scenario}')
            print(f'- Device Count: {data[\"device_count\"]}')
            print(f'- Concurrent Upgrades: {data[\"concurrent_upgrades\"]}')
            print(f'- Startup Time: {data[\"startup_time_seconds\"]}s')
            print(f'- Simulation Time: {data[\"simulation_time_seconds\"]}s')
            print(f'- Memory Usage: {data[\"memory_used_mb\"]}MB')
            print(f'- Peak Memory: {data[\"peak_memory_mb\"]}MB')
            print(f'- Success Rate: {data[\"success_rate\"]}%')
            print('')
          
          # Check performance thresholds
          print('## Performance Analysis')
          large_scale = results.get('Large Scale', {})
          
          if large_scale.get('peak_memory_mb', 0) < 512:
            print('✅ Memory usage within acceptable limits (<512MB)')
          else:
            print('⚠️  Memory usage exceeds recommended limits')
          
          if large_scale.get('simulation_time_seconds', 0) < 120:
            print('✅ Simulation time within acceptable limits (<2min)')
          else:
            print('⚠️  Simulation time exceeds recommended limits')
          
          if large_scale.get('success_rate', 0) >= 95:
            print('✅ Success rate meets requirements (≥95%)')
          else:
            print('⚠️  Success rate below requirements')
          " > performance-benchmark-report.md

      - name: Upload performance report
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmark-report-${{ github.run_number }}
          path: performance-benchmark-report.md

  generate-qa-summary:
    name: Generate QA Test Summary
    runs-on: ubuntu-latest
    needs: [network-error-simulation, stress-testing, performance-benchmarking]
    if: always()
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: qa-artifacts

      - name: Generate comprehensive QA report
        run: |
          echo "# QA Framework Test Summary" > qa-summary-report.md
          echo "Generated: $(date)" >> qa-summary-report.md
          echo "Test Scope: ${{ github.event.inputs.test_scope || 'full' }}" >> qa-summary-report.md
          echo "Device Count: ${{ github.event.inputs.device_count || '100' }}" >> qa-summary-report.md
          echo "Stress Multiplier: ${{ github.event.inputs.stress_multiplier || '1' }}x" >> qa-summary-report.md
          echo "" >> qa-summary-report.md
          
          echo "## Test Results Summary" >> qa-summary-report.md
          
          # Analyze job results
          if [ "${{ needs.network-error-simulation.result }}" = "success" ]; then
            echo "✅ Error Simulation Tests: PASSED" >> qa-summary-report.md
          else
            echo "❌ Error Simulation Tests: FAILED" >> qa-summary-report.md
          fi
          
          if [ "${{ needs.stress-testing.result }}" = "success" ]; then
            echo "✅ Stress Testing: PASSED" >> qa-summary-report.md
          elif [ "${{ needs.stress-testing.result }}" = "skipped" ]; then
            echo "⏭️  Stress Testing: SKIPPED" >> qa-summary-report.md
          else
            echo "❌ Stress Testing: FAILED" >> qa-summary-report.md
          fi
          
          if [ "${{ needs.performance-benchmarking.result }}" = "success" ]; then
            echo "✅ Performance Benchmarking: PASSED" >> qa-summary-report.md
          elif [ "${{ needs.performance-benchmarking.result }}" = "skipped" ]; then
            echo "⏭️  Performance Benchmarking: SKIPPED" >> qa-summary-report.md
          else
            echo "❌ Performance Benchmarking: FAILED" >> qa-summary-report.md
          fi
          
          echo "" >> qa-summary-report.md
          echo "## Artifacts Generated" >> qa-summary-report.md
          echo "The following test artifacts are available for download:" >> qa-summary-report.md
          echo "" >> qa-summary-report.md
          
          # List available artifacts
          find qa-artifacts -name "*.md" -o -name "*.log" -o -name "*.json" | while read file; do
            basename_file=$(basename "$file")
            echo "- \`$basename_file\`" >> qa-summary-report.md
          done
          
          echo "" >> qa-summary-report.md
          echo "## Next Steps" >> qa-summary-report.md
          echo "- Review any failed tests and investigate root causes" >> qa-summary-report.md
          echo "- Check performance benchmarks against requirements" >> qa-summary-report.md
          echo "- Update mock device configurations based on test results" >> qa-summary-report.md
          echo "- Consider running UAT suite if all QA tests pass" >> qa-summary-report.md

      - name: Upload QA summary report
        uses: actions/upload-artifact@v4
        with:
          name: qa-framework-summary-${{ github.run_number }}
          path: qa-summary-report.md